# PipeLine-Immobilier
Projet lors de la formation PMN  2025


---

# Pipeline Immobilier â€“ DVF

Pipeline de collecte â†’ nettoyage â†’ visualisation des donnÃ©es immobiliÃ¨res (type **DVF**) avec un tableau de bord **Streamlit**.

* **`src/spider.py`** : collecte (scraping / tÃ©lÃ©chargement) â†’ produit un **JSON brut**
* **`src/cleaner.py`** : normalisation & enrichissement â†’ produit un **CSV propre**
* **`src/app.py`** : application **Streamlit** pour explorer les donnÃ©es (KPI, carte, filtres)
* **GitHub Actions** : automatisation possible du pipeline & des donnÃ©es publiÃ©es dans `data/`

---

## ğŸ“¦ Arborescence

```
PIPELINE-IMMOBILIER - DVF/
â”œâ”€ .github/
â”‚  â””â”€ workflows/
â”‚     â””â”€ main.yml                 # pipeline CI/CD (optionnel)
â”œâ”€ data/
â”‚  â”œâ”€ cleaned_data.csv            # donnÃ©es nettoyÃ©es (source du dashboard)
â”‚  â”œâ”€ raw_data.json               # donnÃ©es brutes
â”‚  â””â”€ __init__.py
â”œâ”€ src/
â”‚  â”œâ”€ app.py                      # application Streamlit
â”‚  â”œâ”€ cleaner.py                  # script de nettoyage / normalisation
â”‚  â””â”€ spider.py                   # script dâ€™acquisition (scraper / fetch)
â”œâ”€ requirements.txt               # dÃ©pendances Python
â””â”€ README.md
```

---

## âœ¨ FonctionnalitÃ©s principales

* **Acquisition** : construction dâ€™un fichier brut `raw_data.json` (DVF ou autre source)
* **Nettoyage** :

  * Harmonisation des colonnes : `price_eur`, `surface_m2`, `lat`, `lon`, `address`, `postal_code`, `city`
  * Calcul `price_per_m2` quand pertinent
  * Correction automatique des coordonnÃ©es frÃ©quentes :

    * micro-degrÃ©s â†’ **division par 1e6 / 1e5**
    * inversion **lat/lon** (dÃ©tection heuristique France)
* **Visualisation** (Streamlit) :

  * KPIs globaux & filtrÃ©s
  * **Histogramme** des prix
  * **Carte PyDeck** centrÃ©e sur **Ãle-de-France** (filtrage bbox IdF), points en **pixels** (ne grossissent pas au zoom)
  * **Tooltip** : Ville, Adresse, Prix
  * **Filtres** :

    * Prix (â‚¬) : **slider** 
    * Surface (mÂ²), Ville (multiselect), recherche dans lâ€™adresse
  * Bouton **â†» Recharger les donnÃ©es** (vide cache Streamlit **et** contourne le cache CDN GitHub)
* **Robustesse GitHub** :

  * Conversion automatique `github.com/.../blob/...` â†’ `raw.githubusercontent.com/...`
  * RÃ©cupÃ©ration de la **date du dernier commit** (pour afficher â€œdonnÃ©es mises Ã  jour le â€¦â€ et casser les caches)

---

## ğŸ› ï¸ Installation

> Python **3.10+** recommandÃ©.

```bash
python -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸš€ DÃ©marrage rapide

### 1) Lancer le dashboard (avec le CSV fourni)

```bash
streamlit run src/app.py
```

Par dÃ©faut, lâ€™app charge `data/cleaned_data.csv` (ou une URL GitHub *raw* si configurÃ©e dans lâ€™UI).
Dans la barre latÃ©rale, tu peux coller une autre URL *raw* GitHub pour tester un fichier Ã  jour.

### 2) ExÃ©cuter le pipeline complet

> Les scripts `spider.py` et `cleaner.py` peuvent proposer des options.
> Consulte **lâ€™aide intÃ©grÃ©e** :

```bash
python src/spider.py -h
python src/cleaner.py -h
```

Exemple de flux (Ã  adapter Ã  tes options rÃ©elles) :

```bash
# 1) Acquisition (Ã©crit data/raw_data.json)
python src/spider.py --city "Paris" --year-min 2019 --year-max 2024 -o data/raw_data.json

# 2) Nettoyage / normalisation (Ã©crit data/cleaned_data.csv)
python src/cleaner.py -i data/raw_data.json -o data/cleaned_data.csv
```

Ensuite, relance lâ€™app Streamlit ou clique **â†» Recharger les donnÃ©es**.


## ğŸš€ DÃ©marrage du dashboard automatisÃ©

Lien streamlit : https://pipeline-immobilier-dvf-ernest-maryline.streamlit.app/

---
## âš™ï¸ Configuration & variables

### Secrets (facultatif mais recommandÃ©)

* `GITHUB_TOKEN` : augmente la fiabilitÃ©/rapiditÃ© pour rÃ©cupÃ©rer la date du dernier commit via lâ€™API GitHub
  (utile si tu dÃ©ploies sur Streamlit Cloud ou si tu fais beaucoup dâ€™actualisations).

### ParamÃ¨tres carte (dans `src/app.py`)

* Centre & emprise **Ãle-de-France** :

  ```python
  IDF_CENTER_LAT, IDF_CENTER_LON = 48.8566, 2.3522
  IDF_BBOX = dict(lat_min=48.0, lat_max=49.2, lon_min=1.3, lon_max=3.6)
  ```

  Modifie ces bornes si tu souhaites Ã©largir/rÃ©duire lâ€™affichage.

### Colonnes attendues (aprÃ¨s nettoyage)

| Colonne        | Description                                 |
| -------------- | ------------------------------------------- |
| `price_eur`    | Prix en euros (int/float)                   |
| `surface_m2`   | Surface en mÂ²                               |
| `lat`, `lon`   | CoordonnÃ©es gÃ©ographiques en **degrÃ©s**     |
| `address`      | Adresse lisible (tooltip)                   |
| `postal_code`  | Code postal (5 chiffres)                    |
| `city`         | Ville affichÃ©e au format **`Ville (CP)`**   |
| `price_per_m2` | CalculÃ© quand `price_eur` & `surface_m2` OK |

> Si `city` est manquante, elle est reconstruite depuis `address` :
> ex. â€œParis 16e Arrondissement (75116)â€ â†’ **â€œParis (75116)â€**.

---

## ğŸ¤– IntÃ©gration continue (GitHub Actions)

Le fichier `.github/workflows/main.yml` peut :

* Installer les dÃ©pendances
* Lancer `spider.py` puis `cleaner.py`
* Commiter/pousser le nouveau `data/cleaned_data.csv` (selon ta config)

> Ouvre `main.yml` et adapte les jobs (triggers `on:` push/schedule, secrets, etc.).
> **Astuce** : pour commiter depuis lâ€™action, configure `permissions: contents: write` et utilise `actions/checkout` + `git` CLI.

---

## ğŸ—ºï¸ Points importants sur la carte

* La couche utilise `radius_units="pixels"` â†’ taille **constante** Ã  lâ€™Ã©cran (ne grossit pas au zoom).
* Tu peux ajouter un slider â€œTaille des points (px)â€ dans la sidebar si besoin.
* Les points hors IdF sont **ignorÃ©s** Ã  lâ€™affichage (filtre bbox).
  Si certains biens lÃ©gitimes disparaissent, Ã©largis `IDF_BBOX`.

---

## ğŸ§¼ StratÃ©gies de qualitÃ© des donnÃ©es

* **Micro-degrÃ©s** dÃ©tectÃ©s automatiquement â†’ division `1e6` (ou `1e5`)
* **Lat/Lon inversÃ©s** dÃ©tectÃ©s (mÃ©diane latâ‰ˆ2.x & lonâ‰ˆ48.x en France) â†’ swap
* **DÃ©doublonnage optionnel** par coordonnÃ©es exactes :
  dÃ©commente dans `normalize_columns()` si tu veux Ã©viter les points superposÃ©s :

  ```python
  # df = df.drop_duplicates(subset=["lat", "lon"], keep="first")
  ```

---

## ğŸ§¯ DÃ©pannage

* **Je vois encore dâ€™anciennes colonnes/valeurs**
  â†’ clique **â†» Recharger les donnÃ©es** (vide `st.cache_data` **et** contourne le CDN GitHub).
  En dev : `streamlit cache clear`.

* **La carte nâ€™apparaÃ®t pas**
  â†’ vÃ©rifie quâ€™il existe des lignes avec `lat` **et** `lon` valides **et** dans la bbox IdF.
  Ajoute temporairement dans lâ€™app :

  ```python
  st.caption(f"lat [{dff['lat'].min():.5f} .. {dff['lat'].max():.5f}] | "
             f"lon [{dff['lon'].min():.5f} .. {dff['lon'].max():.5f}]")
  ```

* **`{address}` ne sâ€™affiche pas dans le tooltip**
  â†’ assure-toi que la colonne `address` est bien incluse dans le `map_df` passÃ© Ã  la couche PyDeck.


---

## ğŸ§ª Tests manuels rapides

1. Ouvre lâ€™app â†’ vÃ©rifie que les KPI & lâ€™histogramme sâ€™affichent.
2. Recherche â€œBoulevardâ€, â€œGambettaâ€â€¦ â†’ la liste/ carte doit se filtrer.
3. Clique **â†» Recharger les donnÃ©es** aprÃ¨s avoir mis Ã  jour `cleaned_data.csv` sur GitHub â†’ les KPI doivent Ã©voluer.

---

## ğŸ“œ Licence & crÃ©dits

* DonnÃ©es DVF (Etalab) â€” respecte les termes dâ€™usage des donnÃ©es publiques.
* Code : choisis la licence adaptÃ©e (MIT conseillÃ© si open-source).

---

## ğŸ§­ Feuille de route (idÃ©es)

* Export CSV/XLSX des filtres courants
* Selecteur **dâ€™annÃ©e** / pÃ©riode
* Clustering des points sur la carte
* Comparaison arrondissement / ville
* DÃ©ploiement Streamlit Cloud / Docker

---

